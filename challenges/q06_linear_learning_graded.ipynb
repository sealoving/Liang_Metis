{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression and Learning Curve challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 1\n",
    "Generate (fake) data that is linearly related to log(x).\n",
    "\n",
    "You are making this model up. It is of the form B0 + B1*log(x) + epsilon. (You are making up the parameters.)\n",
    "\n",
    "Simulate some data from this model.\n",
    "\n",
    "Then fit two models to it:\n",
    "\n",
    "quadratic (second degree polynomial)\n",
    "\n",
    "logarithmic (log(x))\n",
    "(The second one should fit really well, since it has the same form as the underlying model!)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "def f(x):\n",
    "    return 10+3*np.log(x)\n",
    "\n",
    "# generate points used to plot\n",
    "# This returns 100 evenly spaced numbers from 0 to 1\n",
    "x_plot = np.linspace(2, 50, 100)\n",
    "\n",
    "# generate points and keep a subset of them\n",
    "n_samples = 100\n",
    "# Generate the x values from the random uniform distribution between 0 and 1\n",
    "X = np.random.uniform(x_plot.min(), x_plot.max(), size=n_samples)[:, np.newaxis]\n",
    "# Generate the y values by taking the sin and adding a random Gaussian (normal) noise term\n",
    "y = f(X) + np.random.normal(scale=0.5, size=n_samples)[:, np.newaxis] #JB you mentioned sine here, but this pulls from log model, FYI\n",
    "\n",
    "# Plot the training data against what we know to be the ground truth sin function\n",
    "fig,ax = plt.subplots(1,1);\n",
    "ax.plot(x_plot, f(x_plot), label='ground truth', color='m')\n",
    "ax.scatter(X, y, label='data', s=20, facecolors=(0,0.9,1),edgecolors=(0,0,0))\n",
    "# ax.set_ylim((0, 2))\n",
    "# ax.set_xlim((0, 1))\n",
    "ax.set_ylabel('y')\n",
    "ax.set_xlabel('x')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import PolynomialFeatures and make_pipeline for Polynomial Regression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Plot the results of a pipeline against ground truth and actual data\n",
    "def plot_approximation(est, ax, label=None):\n",
    "    \"\"\"Plot the approximation of ``est`` on axis ``ax``. \"\"\"\n",
    "    ax.plot(x_plot, f(x_plot), label='ground truth', color='green')\n",
    "    ax.scatter(X, y, s=100)\n",
    "    ax.plot(x_plot, est.predict(x_plot[:, np.newaxis]), color='red', label=label)\n",
    "#     ax.set_ylim((-2, 2))\n",
    "#     ax.set_xlim((0, 1))\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_xlabel('x')\n",
    "    ax.legend(loc='lower right',frameon=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the plot\n",
    "fig,ax = plt.subplots(1,1)\n",
    "# Set the degree of our polynomial\n",
    "degree = 2\n",
    "# Generate the model type with make_pipeline\n",
    "# This tells it the first step is to generate 3rd degree polynomial features in the input features and then run\n",
    "# a linear regression on the resulting features\n",
    "est = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
    "# Fit our model to the training data\n",
    "est.fit(X, y)\n",
    "# Plot the results\n",
    "plot_approximation(est, ax, label='poly%d fit' % degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est.score(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "transformer = FunctionTransformer(np.log)\n",
    "est = make_pipeline(transformer, LinearRegression())\n",
    "est.fit(X, y)\n",
    "# Plot the results\n",
    "fig,ax = plt.subplots(1,1)\n",
    "plot_approximation(est, ax, label='log fit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est.score(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2\n",
    "Generate (fake) data from a model of the form B0 + B1*x + B2*x^2 + epsilon. (You are making up the parameters.)\n",
    "\n",
    "Split the data into a training and test set.\n",
    "\n",
    "Fit a model to your training set. Calculate mean squared error on your training set. Then calculate it on your test set.\n",
    "\n",
    "(You could use sklearn.metrics.mean_squared_error.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "def f(x):\n",
    "    return 5+3*x+0.5*x**2\n",
    "\n",
    "# generate points used to plot\n",
    "# This returns 100 evenly spaced numbers from 0 to 1\n",
    "x_plot = np.linspace(-10, 5, 100)\n",
    "\n",
    "# generate points and keep a subset of them\n",
    "n_samples = 100\n",
    "# Generate the x values from the random uniform distribution between 0 and 1\n",
    "X = np.random.uniform(x_plot.min(), x_plot.max(), size=n_samples)[:, np.newaxis]\n",
    "# Generate the y values by taking the sin and adding a random Gaussian (normal) noise term\n",
    "y = f(X) + np.random.normal(scale=5, size=n_samples)[:, np.newaxis]\n",
    "\n",
    "# Plot the training data against what we know to be the ground truth sin function\n",
    "fig,ax = plt.subplots(1,1);\n",
    "ax.plot(x_plot, f(x_plot), label='ground truth', color='m')\n",
    "ax.scatter(X, y, label='data', s=20, facecolors=(0,0.9,1),edgecolors=(0,0,0))\n",
    "# ax.set_ylim((0, 2))\n",
    "# ax.set_xlim((0, 1))\n",
    "ax.set_ylabel('y')\n",
    "ax.set_xlabel('x')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree = 2\n",
    "est = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
    "# INSTRUCTOR NOTE: Run this multiple times to show the variation\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=4444,test_size=0.3)\n",
    "# Fit the model against the training data\n",
    "est.fit(X_train, y_train)\n",
    "# Evaluate the model against the testing data\n",
    "est.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr = LinearRegression() #JB added these, as I'm not sure how you were fitting otherwise\n",
    "lr.fit(X_train, y_train) #JB added these, as I'm not sure how you were fitting otherwise\n",
    "\n",
    "print('training error (MSE)', metrics.mean_squared_error(lr.predict(X_train),y_train))\n",
    "print('test error (MSE):', metrics.mean_squared_error(lr.predict(X_test),y_test))\n",
    "print('trainning R2:', lr.score(X_train,y_train))\n",
    "print('test R2:', lr.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3\n",
    "For the data from two (above), try polynomial fits from 0th (just constant) to 7th order (highest term x^7). Over the x axis of model degree (8 points), plot:\n",
    "\n",
    "training error  \n",
    "test error  \n",
    "R squared  \n",
    "AIC  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step through degrees from 0 to 9 and store the training and test (generalization) error.\n",
    "# This sets up 5 rows of 2 plots each (KEEP)\n",
    "fig, ax = plt.subplots(2, 2, figsize=(15, 10))\n",
    "degrees = []\n",
    "train_err=[]\n",
    "test_err=[]\n",
    "R2 = []\n",
    "AIC = []\n",
    "for degree in range(8):\n",
    "    est = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
    "    est.fit(X_train, y_train)\n",
    "    degrees.append(degree)\n",
    "    train_err.append(metrics.mean_squared_error(est.predict(X_train),y_train))\n",
    "    test_err.append(metrics.mean_squared_error(est.predict(X_test),y_test))\n",
    "    R2.append(est.score(X_test,y_test))\n",
    "ax[0,0].scatter(degrees,train_err)\n",
    "ax[1,0].scatter(degrees,test_err)\n",
    "ax[0,1].scatter(degrees,R2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 4\n",
    "For the data from two (above), fit a model to only the first 5 of your data points (m=5). Then to first 10 (m=10). Then to first 15 (m=15). In this manner, keep fitting until you fit your entire training set. For each step, calculate the training error and the test error. Plot both (in the same plot) over m. This is called a learning curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "ms = []\n",
    "train_err=[]\n",
    "test_err=[]\n",
    "R2 = []\n",
    "AIC = []\n",
    "degree = 2\n",
    "est = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
    "for m in np.arange(5,100,5):\n",
    "    X_train = X[:m]\n",
    "    y_train = y[:m]\n",
    "    X_test = X[m:]\n",
    "    y_test = y[m:]\n",
    "    est.fit(X_train, y_train)\n",
    "    ms.append(m)\n",
    "    train_err.append(metrics.mean_squared_error(est.predict(X_train),y_train))\n",
    "    test_err.append(metrics.mean_squared_error(est.predict(X_test),y_test))\n",
    "    R2.append(est.score(X_test,y_test))\n",
    "ax.plot(ms,train_err)\n",
    "ax.plot(ms,test_err)\n",
    "\n",
    "#JB make sure to add a legend so we know what we're looking at, like below\n",
    "plt.legend(('Train Error', 'Test Error'),\n",
    "           shadow=True, loc=(0.7, 0.55))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": false,
   "nav_menu": {
    "height": "102px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": false,
   "threshold": "3",
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
