{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get cleaned transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2467\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "      <th>description</th>\n",
       "      <th>duration</th>\n",
       "      <th>event</th>\n",
       "      <th>film_date</th>\n",
       "      <th>languages</th>\n",
       "      <th>main_speaker</th>\n",
       "      <th>name</th>\n",
       "      <th>num_speaker</th>\n",
       "      <th>published_date</th>\n",
       "      <th>ratings</th>\n",
       "      <th>related_talks</th>\n",
       "      <th>speaker_occupation</th>\n",
       "      <th>tags</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>views</th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4553</td>\n",
       "      <td>Sir Ken Robinson makes an entertaining and pro...</td>\n",
       "      <td>1164</td>\n",
       "      <td>TED2006</td>\n",
       "      <td>1140825600</td>\n",
       "      <td>60</td>\n",
       "      <td>Ken Robinson</td>\n",
       "      <td>Ken Robinson: Do schools kill creativity?</td>\n",
       "      <td>1</td>\n",
       "      <td>1151367060</td>\n",
       "      <td>[{'id': 7, 'name': 'Funny', 'count': 19645}, {...</td>\n",
       "      <td>[{'id': 865, 'hero': 'https://pe.tedcdn.com/im...</td>\n",
       "      <td>Author/educator</td>\n",
       "      <td>['children', 'creativity', 'culture', 'dance',...</td>\n",
       "      <td>Do schools kill creativity?</td>\n",
       "      <td>https://www.ted.com/talks/ken_robinson_says_sc...</td>\n",
       "      <td>47227110</td>\n",
       "      <td>it been great hasnt it. ive been blown away by...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   comments                                        description  duration  \\\n",
       "0      4553  Sir Ken Robinson makes an entertaining and pro...      1164   \n",
       "\n",
       "     event   film_date  languages  main_speaker  \\\n",
       "0  TED2006  1140825600         60  Ken Robinson   \n",
       "\n",
       "                                        name  num_speaker  published_date  \\\n",
       "0  Ken Robinson: Do schools kill creativity?            1      1151367060   \n",
       "\n",
       "                                             ratings  \\\n",
       "0  [{'id': 7, 'name': 'Funny', 'count': 19645}, {...   \n",
       "\n",
       "                                       related_talks speaker_occupation  \\\n",
       "0  [{'id': 865, 'hero': 'https://pe.tedcdn.com/im...    Author/educator   \n",
       "\n",
       "                                                tags  \\\n",
       "0  ['children', 'creativity', 'culture', 'dance',...   \n",
       "\n",
       "                         title  \\\n",
       "0  Do schools kill creativity?   \n",
       "\n",
       "                                                 url     views  \\\n",
       "0  https://www.ted.com/talks/ken_robinson_says_sc...  47227110   \n",
       "\n",
       "                                          transcript  \n",
       "0  it been great hasnt it. ive been blown away by...  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all = pd.read_pickle('./data/df_all_lemma.pkl')\n",
    "print(len(df_all))\n",
    "df_all.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['alternative energy', 'cars', 'climate change', 'culture', 'environment', 'global issues', 'science', 'sustainability', 'technology']\""
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.iloc[1].tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'it been great hasnt it. ive been blown away by the whole thing. in fact im leaving. there have been three theme running through the conference which are relevant to what i want to talk about. one is the extraordinary evidence of human creativity in all of the presentation that weve had and in all of the people here. just the variety of it and the range of it. the second is that it put u in a place where we have no idea whats going to happen in term of the future. no idea how this may play out. i have an interest in education. actually what i find is everybody ha an interest in education. dont you. i find this very interesting. if youre at a dinner party and you say you work in education actually youre not often at dinner party frankly. if you work in education youre not asked. and youre never asked back curiously. thats strange to me. but if you are and you say to somebody you know they say what do you do. and you say you work in education you can see the blood run from their face. theyre like oh my god you know why me. my one night out all week. but if you ask about their education they pin you to the wall. because it one of those thing that go deep with people am i right. like religion and money and other thing. so i have a big interest in education and i think we all do. we have a huge vested interest in it partly because it education thats meant to take u into this future that we cant grasp. if you think of it child starting school this year will be retiring in. nobody ha a clue despite all the expertise thats been on parade for the past four day what the world will look like in five year time. and yet were meant to be educating them for it. so the unpredictability i think is extraordinary. and the third part of this is that weve all agreed nonetheless on the really extraordinary capacity that child have their capacity for innovation. i mean sirena last night wa a marvel wasnt she. just seeing what she could do. and shes exceptional but i think shes not so to speak exceptional in the whole of childhood. what you have there is a person of extraordinary dedication who found a talent. and my contention is all kid have tremendous talent. and we squander them pretty ruthlessly. so i want to talk about education and i want to talk about creativity. my contention is that creativity now is a important in education a literacy and we should treat it with the same status. thank you. that wa it by the way. thank you very much. so minute left. well i wa born. . . no. i heard a great story recently i love telling it of a little girl who wa in a drawing lesson. she wa six and she wa at the back drawing and the teacher said this girl hardly ever paid attention and in this drawing lesson she did. the teacher wa fascinated. she went over to her and she said what are you drawing. and the girl said im drawing a picture of god. and the teacher said but nobody know what god look like. and the girl said they will in a minute. when my son wa four in england actually he wa four everywhere to be honest. if were being strict about it wherever he went he wa four that year. he wa in the nativity play. do you remember the story. no it wa big it wa a big story. mel gibson did the sequel you may have seen it. nativity ii. but james got the part of joseph which we were thrilled about. we considered this to be one of the lead part. we had the place crammed full of agent in tshirts james robinson is joseph. he didnt have to speak but you know the bit where the three king come in. they come in bearing gift gold frankincense and myrrh. this really happened. we were sitting there and i think they just went out of sequence because we talked to the little boy afterward and we said you ok with that. and he said yeah why. wa that wrong. they just switched. the three boy came in fouryearolds with tea towel on their head and they put these box down and the first boy said i bring you gold. and the second boy said i bring you myrrh. and the third boy said frank sent this. what these thing have in common is that kid will take a chance. if they dont know theyll have a go. am i right. theyre not frightened of being wrong. i dont mean to say that being wrong is the same thing a being creative. what we do know is if youre not prepared to be wrong youll never come up with anything original if youre not prepared to be wrong. and by the time they get to be adult most kid have lost that capacity. they have become frightened of being wrong. and we run our company like this. we stigmatize mistake. and were now running national education system where mistake are the worst thing you can make. and the result is that we are educating people out of their creative capacity. picasso once said this he said that all child are born artist. the problem is to remain an artist a we grow up. i believe this passionately that we dont grow into creativity we grow out of it. or rather we get educated out of it. so why is this. i lived in stratfordonavon until about five year ago. in fact we moved from stratford to los angeles. so you can imagine what a seamless transition that wa. actually we lived in a place called snitterfield just outside stratford which is where shakespeare father wa born. are you struck by a new thought. i wa. you dont think of shakespeare having a father do you. do you. because you dont think of shakespeare being a child do you. shakespeare being seven. i never thought of it. i mean he wa seven at some point. he wa in somebody english class wasnt he. how annoying would that be. must try harder. being sent to bed by his dad you know to shakespeare go to bed now. and put the pencil down. and stop speaking like that. it confusing everybody. anyway we moved from stratford to los angeles and i just want to say a word about the transition. my son didnt want to come. ive got two kid he now my daughter. he didnt want to come to los angeles. he loved it but he had a girlfriend in england. this wa the love of his life sarah. hed known her for a month. mind you theyd had their fourth anniversary because it a long time when youre. he wa really upset on the plane he said ill never find another girl like sarah. and we were rather pleased about that frankly because she wa the main reason we were leaving the country. but something strike you when you move to america and travel around the world every education system on earth ha the same hierarchy of subject. every one. doesnt matter where you go. youd think it would be otherwise but it isnt. at the top are mathematics and language then the humanity and at the bottom are the art. everywhere on earth. and in pretty much every system too there a hierarchy within the art. art and music are normally given a higher status in school than drama and dance. there isnt an education system on the planet that teach dance everyday to child the way we teach them mathematics. why. why not. i think this is rather important. i think math is very important but so is dance. child dance all the time if theyre allowed to we all do. we all have body dont we. did i miss a meeting. truthfully what happens is a child grow up we start to educate them progressively from the waist up. and then we focus on their head. and slightly to one side. if you were to visit education a an alien and say whats it for public education. i think youd have to conclude if you look at the output who really succeeds by this who doe everything that they should who get all the brownie point who are the winner i think youd have to conclude the whole purpose of public education throughout the world is to produce university professor. isnt it. theyre the people who come out the top. and i used to be one so there. and i like university professor but you know we shouldnt hold them up a the highwater mark of all human achievement. theyre just a form of life another form of life. but theyre rather curious and i say this out of affection for them. there something curious about professor in my experience not all of them but typically they live in their head. they live up there and slightly to one side. theyre disembodied you know in a kind of literal way. they look upon their body a a form of transport for their head. dont they. it a way of getting their head to meeting. if you want real evidence of outofbody experience get yourself along to a residential conference of senior academic and pop into the discotheque on the final night. and there you will see it. grown men and woman writhing uncontrollably off the beat. waiting until it end so they can go home and write a paper about it. our education system is predicated on the idea of academic ability. and there a reason. around the world there were no public system of education really before the th century. they all came into being to meet the need of industrialism. so the hierarchy is rooted on two idea. number one that the most useful subject for work are at the top. so you were probably steered benignly away from thing at school when you were a kid thing you liked on the ground that you would never get a job doing that. is that right. dont do music youre not going to be a musician dont do art you wont be an artist. benign advice now profoundly mistaken. the whole world is engulfed in a revolution. and the second is academic ability which ha really come to dominate our view of intelligence because the university designed the system in their image. if you think of it the whole system of public education around the world is a protracted process of university entrance. and the consequence is that many highlytalented brilliant creative people think theyre not because the thing they were good at at school wasnt valued or wa actually stigmatized. and i think we cant afford to go on that way. in the next year according to unesco more people worldwide will be graduating through education than since the beginning of history. more people and it the combination of all the thing weve talked about technology and it transformation effect on work and demography and the huge explosion in population. suddenly degree arent worth anything. isnt that true. when i wa a student if you had a degree you had a job. if you didnt have a job it because you didnt want one. and i didnt want one frankly. but now kid with degree are often heading home to carry on playing video game because you need an ma where the previous job required a ba and now you need a phd for the other. it a process of academic inflation. and it indicates the whole structure of education is shifting beneath our foot. we need to radically rethink our view of intelligence. we know three thing about intelligence. one it diverse. we think about the world in all the way that we experience it. we think visually we think in sound we think kinesthetically. we think in abstract term we think in movement. secondly intelligence is dynamic. if you look at the interaction of a human brain a we heard yesterday from a number of presentation intelligence is wonderfully interactive. the brain isnt divided into compartment. in fact creativity which i define a the process of having original idea that have value more often than not come about through the interaction of different disciplinary way of seeing thing. by the way there a shaft of nerve that join the two half of the brain called the corpus callosum. it thicker in woman. following off from helen yesterday this is probably why woman are better at multitasking. because you are arent you. there a raft of research but i know it from my personal life. if my wife is cooking a meal at home which is not often thankfully. no shes good at some thing but if shes cooking shes dealing with people on the phone shes talking to the kid shes painting the ceiling shes doing openheart surgery over here. if im cooking the door is shut the kid are out the phone on the hook if she come in i get annoyed. i say terry please im trying to fry an egg in here. give me a break. actually do you know that old philosophical thing if a tree fall in a forest and nobody hears it did it happen. remember that old chestnut. i saw a great tshirt recently which said if a man speaks his mind in a forest and no woman hears him is he still wrong. and the third thing about intelligence is it distinct. im doing a new book at the moment called epiphany which is based on a series of interview with people about how they discovered their talent. im fascinated by how people got to be there. it really prompted by a conversation i had with a wonderful woman who maybe most people have never heard of gillian lynne. have you heard of her. some have. shes a choreographer and everybody know her work. she did cat and phantom of the opera. shes wonderful. i used to be on the board of the royal ballet a you can see. anyway gillian and i had lunch one day and i said how did you get to be a dancer. it wa interesting. when she wa at school she wa really hopeless. and the school in the s wrote to her parent and said we think gillian ha a learning disorder. she couldnt concentrate she wa fidgeting. i think now theyd say she had adhd. wouldnt you. but this wa the s and adhd hadnt been invented at this point. it wasnt an available condition. people werent aware they could have that. anyway she went to see this specialist. so this oakpaneled room and she wa there with her mother and she wa led and sat on this chair at the end and she sat on her hand for minute while this man talked to her mother about the problem gillian wa having at school. because she wa disturbing people her homework wa always late and so on little kid of eight. in the end the doctor went and sat next to gillian and said ive listened to all these thing your mother told me i need to speak to her privately. wait here. well be back we wont be very long and they went and left her. but a they went out of the room he turned on the radio that wa sitting on his desk. and when they got out he said to her mother just stand and watch her. and the minute they left the room she wa on her foot moving to the music. and they watched for a few minute and he turned to her mother and said mr lynne gillian isnt sick shes a dancer. take her to a dance school. i said what happened. she said she did. i cant tell you how wonderful it wa. we walked in this room and it wa full of people like me. people who couldnt sit still. people who had to move to think. who had to move to think. they did ballet they did tap jazz they did modern they did contemporary. she wa eventually auditioned for the royal ballet school she became a soloist she had a wonderful career at the royal ballet. she eventually graduated from the royal ballet school founded the gillian lynne dance company met andrew lloyd webber. shes been responsible for some of the most successful musical theater production in history shes given pleasure to million and shes a multimillionaire. somebody else might have put her on medication and told her to calm down. what i think it come to is this al gore spoke the other night about ecology and the revolution that wa triggered by rachel carson. i believe our only hope for the future is to adopt a new conception of human ecology one in which we start to reconstitute our conception of the richness of human capacity. our education system ha mined our mind in the way that we stripmine the earth for a particular commodity. and for the future it wont serve u. we have to rethink the fundamental principle on which were educating our child. there wa a wonderful quote by jonas salk who said if all the insect were to disappear from the earth within year all life on earth would end. if all human being disappeared from the earth within year all form of life would flourish. and he right. what ted celebrates is the gift of the human imagination. we have to be careful now that we use this gift wisely and that we avert some of the scenario that weve talked about. and the only way well do it is by seeing our creative capacity for the richness they are and seeing our child for the hope that they are. and our task is to educate their whole being so they can face this future. by the way we may not see this future but they will. and our job is to help them make something of it'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.transcript[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147\n"
     ]
    }
   ],
   "source": [
    "# select talks tagged with \"education\"\n",
    "df_edu = df_all[df_all['tags'].apply(lambda x: 'education' in x)]\n",
    "print(len(df_edu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs_all = list(df_all['transcript'])\n",
    "docs_edu = list(df_edu['transcript'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# docs_edu[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get word count for all talks (stop_all) and for edu talks (stop_edu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Word</th>\n",
       "      <th>Part of speech</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>Dispersion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>the</td>\n",
       "      <td>a</td>\n",
       "      <td>22038615</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rank    Word Part of speech  Frequency  Dispersion\n",
       "0     1     the              a   22038615        0.98"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get most common 5000 words in English\n",
    "df_5000 = pd.read_pickle('./data/common_5000.pkl')\n",
    "common_5000 = set(s[3:] for s in list(df_5000['   Word']))\n",
    "df_5000.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def get_word_count(docs):\n",
    "\n",
    "    # Get the content\n",
    "    content = ' '.join(docs)\n",
    "    content = re.sub('[^A-Za-z ]+', '', content)  # remove non-alpha chars\n",
    "    words = content.split()\n",
    "\n",
    "    # Start counting\n",
    "    word_count = Counter(words)\n",
    "    \n",
    "    return word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get word count for all talks\n",
    "word_count_all = get_word_count(docs_all)\n",
    "# get word count for all edu talks\n",
    "word_count_edu = get_word_count(docs_edu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get most common 5000 words in all TED talks\n",
    "# Get most common 5000 words in all TED-edu talks\n",
    "N = 5000\n",
    "common_all = [word for (word, count) in word_count_all.most_common(N)]\n",
    "common_edu = [word for (word, count) in word_count_edu.most_common(N)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NMF and LDA with standard stop words 'english'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = docs_all\n",
    "\n",
    "# df_temp = df_all[df_all['tags'].apply(lambda x: 'children' in x)]\n",
    "# docs = list(df_temp['transcript'])\n",
    "# len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf-idf features for NMF...\n",
      "done in 0.180s.\n",
      "Extracting tf features for LDA...\n",
      "done in 0.188s.\n",
      "\n",
      "Fitting the NMF model (Frobenius norm) with tf-idf features, n_samples=2000 and n_features=1000...\n",
      "done in 0.055s.\n",
      "\n",
      "Topics in NMF model (Frobenius norm):\n",
      "Topic #0: men money class film stuff building car fear art american\n",
      "Topic #1: baby statistic ball hypothesis language fetus brain milk evidence birth\n",
      "Topic #2: father dance photograph dad type door watched needed wear authority\n",
      "Topic #3: autism individual gene genetic cause disorder diagnosis condition brain identify\n",
      "Topic #4: grade college successful childhood success score summer wait principle principal\n",
      "Topic #5: refugee camp war parenting coach conflict crisis support grandmother journey\n",
      "Topic #6: game project video code scratch engineering write interactive cancer paper\n",
      "Topic #7: english india computer slum experiment educational google area village internet\n",
      "Topic #8: laptop drop project government rest village teaching television certainly scale\n",
      "Topic #9: brain disorder helmet skull adolescence football injury cortex gray adolescent\n",
      "Topic #10: brother yearold song fell pound excited wed wrote key discovered\n",
      "Topic #11: sex sexual pregnancy pregnant men pleasure partner worth conversation desire\n",
      "Topic #12: lunch food lady breakfast feeding dollar meal chicken billion eat\n",
      "Topic #13: nature forest plant specie tree park wild animal national planet\n",
      "Topic #14: water bottle planet content data method global crisis drink sun\n",
      "\n",
      "Fitting the NMF model (generalized Kullback-Leibler divergence) with tf-idf features, n_samples=2000 and n_features=1000...\n",
      "done in 0.658s.\n",
      "\n",
      "Topics in NMF model (generalized Kullback-Leibler divergence):\n",
      "Topic #0: money reading wouldnt everybody built build sitting living dad special\n",
      "Topic #1: pattern view science simple baby childrens completely machine lab result\n",
      "Topic #2: buy hold sick father political understanding worried death suffering worker\n",
      "Topic #3: research window brain gene test simply youd opportunity treatment share\n",
      "Topic #4: solution rate happening poverty money class outcome possible teaching quality\n",
      "Topic #5: yellow watch visit water return business th ground zone basically\n",
      "Topic #6: write familiar wouldnt telling data changed law developing lack behavior\n",
      "Topic #7: taught worked material wed teaching gone wait project realized outside\n",
      "Topic #8: white thousand car figured growth access tool totally opportunity level\n",
      "Topic #9: inside stay exist daughter straight sound language nice content safe\n",
      "Topic #10: yearold opportunity piece wonder brother possibility fight dance wed wrote\n",
      "Topic #11: incredibly zero wife calling happy bunch spend shell york okay\n",
      "Topic #12: run somebody seeing educate phone character breakfast tv theyd connection\n",
      "Topic #13: lie outside dna happening blue scientific watching century water structure\n",
      "Topic #14: picked wont win voice ready wall safe loved chair bunch\n",
      "\n",
      "Fitting LDA models with tf features, n_samples=2000 and n_features=1000...\n",
      "done in 0.582s.\n",
      "\n",
      "Topics in LDA model:\n",
      "Topic #0: water television birth stress pregnancy tv bottle failure global inside\n",
      "Topic #1: soap chemical thomas rocket baby breast exposed health male public\n",
      "Topic #2: game poor orphanage sleep money father playing baby health bank\n",
      "Topic #3: online brain internet network stranger knowing conversation key sex exposed\n",
      "Topic #4: ride vision gene men standing picture building bike project grandmother\n",
      "Topic #5: fear disease research share researcher medical gene blood data health\n",
      "Topic #6: disney data happening autism brain daughter angry black response project\n",
      "Topic #7: dog baby project computer design toy film ball english stuff\n",
      "Topic #8: food science lunch dollar stuff okay bee billion eat spend\n",
      "Topic #9: men gender feminist culture female louis marriage class power monitor\n",
      "Topic #10: building class college grade childhood men code score build gender\n",
      "Topic #11: milk forest fly planet nature water baby data failure structure\n",
      "Topic #12: autism brain baby picture individual period comic gun information water\n",
      "Topic #13: brain laptop lie fetus helmet football skull lying video facial\n",
      "Topic #14: refugee camp war solution living test crisis power history political\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Author: Olivier Grisel <olivier.grisel@ensta.org>\n",
    "#         Lars Buitinck\n",
    "#         Chyi-Kwei Yau <chyikwei.yau@gmail.com>\n",
    "# License: BSD 3 clause\n",
    "\n",
    "from time import time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "n_samples = 2000\n",
    "n_features = 1000\n",
    "n_components = 15\n",
    "n_top_words = 10\n",
    "n_gram = 1\n",
    "alpha = 0.1\n",
    "stop_choice= 'english'\n",
    "\n",
    "max_df = 0.3\n",
    "min_df = 3\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()\n",
    "\n",
    "\n",
    "# Use tf-idf features for NMF.\n",
    "print(\"Extracting tf-idf features for NMF...\")\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=max_df, min_df=min_df,\n",
    "                                   ngram_range=(n_gram,n_gram),\n",
    "                                   max_features=n_features,\n",
    "                                   stop_words=stop_choice)\n",
    "t0 = time()\n",
    "tfidf = tfidf_vectorizer.fit_transform(docs)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "# Use tf (raw term count) features for LDA.\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = CountVectorizer(max_df=max_df, min_df=min_df,\n",
    "                                ngram_range=(n_gram,n_gram),\n",
    "                                max_features=n_features,\n",
    "                                stop_words=stop_choice)\n",
    "t0 = time()\n",
    "tf = tf_vectorizer.fit_transform(docs)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "print()\n",
    "\n",
    "# Fit the NMF model\n",
    "print(\"Fitting the NMF model (Frobenius norm) with tf-idf features, \"\n",
    "      \"n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "t0 = time()\n",
    "nmf = NMF(n_components=n_components, random_state=1,\n",
    "          alpha=alpha, l1_ratio=.5).fit(tfidf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "print(\"\\nTopics in NMF model (Frobenius norm):\")\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words)\n",
    "\n",
    "# Fit the NMF model\n",
    "print(\"Fitting the NMF model (generalized Kullback-Leibler divergence) with \"\n",
    "      \"tf-idf features, n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "t0 = time()\n",
    "nmf = NMF(n_components=n_components, random_state=1,\n",
    "          beta_loss='kullback-leibler', solver='mu', max_iter=1000, alpha=alpha,\n",
    "          l1_ratio=.5).fit(tfidf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "print(\"\\nTopics in NMF model (generalized Kullback-Leibler divergence):\")\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words)\n",
    "\n",
    "print(\"Fitting LDA models with tf features, \"\n",
    "      \"n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "lda = LatentDirichletAllocation(n_components=n_components, max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "t0 = time()\n",
    "lda.fit(tf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "print(\"\\nTopics in LDA model:\")\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# n_samples = 2000\n",
    "# n_features = 1000\n",
    "# n_components = 15\n",
    "# n_top_words = 10\n",
    "# n_gram = 1\n",
    "# alpha = 0.1\n",
    "# stop_choice= 'english'#stop_list[:100]\n",
    "\n",
    "# max_df = 0.3\n",
    "# min_df = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NMF and LDA with customized stop words, based on word counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'and', 'to', 'a', 'of', 'that', 'in', 'it', 'i', 'is', 'you', 'we', 'this', 'so', 'wa', 'for', 'are', 'have', 'but', 'they', 'on', 'with', 'what', 'can', 'about', 'there', 'be', 'not', 'at', 'all', 'my', 'one', 'do', 'people', 'were', 'like', 'if', 'from', 'our', 'or', 'now', 'just', 'these', 'an', 'he', 'when', 'because', 'thing', 'me', 'out', 'by', 'how', 'them', 'know', 'very', 'more', 'up', 'going', 'had', 'think', 'their', 'time', 'who', 'u', 'your', 'get', 'year', 'thats', 'see', 'would', 'which', 'here', 'really', 'way', 'then', 'world', 'some', 'im', 'ha', 'dont', 'make', 'go', 'into', 'will', 'actually', 'where', 'well', 'want', 'could', 'other', 'no', 'right', 'been', 'look', 'life', 'say', 'those', 'said', 'work', 'something', 'first', 'than', 'need', 'also', 'two', 'take', 'she', 'new', 'even', 'come', 'most', 'lot', 'over', 'much', 'got', 'kind', 'little', 'only', 'back', 'his', 'youre', 'day', 'many', 'theyre', 'did', 'every', 'why', 'good', 'around', 'her', 'let', 'different', 'being', 'mean', 'through', 'same', 'down', 'idea', 'human', 'problem', 'use', 'change', 'put', 'tell', 'doing', 'woman', 'story', 'any', 'weve', 'country', 'three', 'percent', 'system', 'place', 'called', 'give', 'after', 'today', 'part', 'made', 'find', 'question', 'fact', 'didnt', 'ive', 'child', 'show', 'talk', 'great', 'own', 'another', 'before', 'started', 'thought', 'start', 'better', 'never', 'should', 'last', 'technology', 'might', 'big', 'brain', 'important', 'together', 'still', 'cant', 'went', 'example', 'able', 'each', 'again', 'next', 'him', 'doe', 'course', 'point', 'number', 'million', 'school', 'off', 'city', 'came', 'bit', 'too', 'few', 'kid', 'used', 'feel', 'love', 'between', 'end', 'data', 'whats', 'help', 'ago', 'maybe', 'call', 'water', 'sort', 'found', 'doesnt', 'state', 'long', 'second', 'word', 'understand', 'looking', 'live', 'done', 'wanted', 'may', 'always', 'real', 'turn', 'person', 'family', 'ever', 'try', 'away', 'believe', 'trying', 'working', 'body', 'whole', 'space', 'guy', 'everything', 'home', 's', 'cell', 'building', 'using', 'hand', 'thinking', 'power', 'four', 'community', 'information', 'company', 'reason', 'money', 'experience', 'took', 'create', 'future', 'computer', 'friend', 'case', 'picture', 'light', 'car', 'enough', 'become', 'man', 'design', 'moment', 'five', 'book', 'getting', 'job', 'without', 'small', 'best', 'project', 'le', 'social', 'dollar', 'youve', 'old', 'group', 'month', 'making', 'sense', 'food', 'probably', 'left', 'ask', 'talking', 'science']\n"
     ]
    }
   ],
   "source": [
    "stops_custom = common_all[:300]# + list(set(common_edu).difference(set(common_5000)))\n",
    "print(stops_custom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stops_standard = stopwords.words('english')\n",
    "stop_list = stops_standard + stops_custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# docs = docs_edu\n",
    "\n",
    "df_temp = df_all[df_all['tags'].apply(lambda x: 'children' in x)]\n",
    "docs = list(df_temp['transcript'])\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf-idf features for NMF...\n",
      "done in 0.161s.\n",
      "Extracting tf features for LDA...\n",
      "done in 0.180s.\n",
      "\n",
      "Fitting the NMF model (Frobenius norm) with tf-idf features, n_samples=2000 and n_features=2000...\n",
      "done in 0.058s.\n",
      "\n",
      "Topics in NMF model (Frobenius norm):\n",
      "Topic #0: men stuff class american father film classroom dream art health\n",
      "Topic #1: baby statistic ball language milk evidence hypothesis fetus detector breast\n",
      "Topic #2: war arm weapon zone notice armed dying aid roughly security\n",
      "Topic #3: autism individual gene disorder genetic diagnosis cause spectrum contact condition\n",
      "Topic #4: college summer scholar grade empowered york principal chore lowincome childhood\n",
      "Topic #5: refugee camp war syria muslim leadership parenting coach conflict displaced\n",
      "Topic #6: game video write cancer award interactive player classroom sequence elementary\n",
      "Topic #7: laptop display drop rest government market village teaching television itll\n",
      "Topic #8: electrical fire intelligent circuit energy injury invention saving prevent shut\n",
      "Topic #9: brother yearold song introduce honor capital closed chose fell explain\n",
      "Topic #10: father dance jail daddy photograph dad type door needed watched\n",
      "Topic #11: lunch lady feeding chicken billion meal feed staff eat nutrition\n",
      "Topic #12: forest nature plant ecosystem specie tree park wild animal national\n",
      "Topic #13: sex sexual pregnancy pregnant men pleasure partner intimate refuse worth\n",
      "Topic #14: institution orphanage cambodia residential support disability living poverty volunteer service\n",
      "\n",
      "Fitting the NMF model (generalized Kullback-Leibler divergence) with tf-idf features, n_samples=2000 and n_features=2000...\n",
      "done in 1.250s.\n",
      "\n",
      "Topics in NMF model (generalized Kullback-Leibler divergence):\n",
      "Topic #0: okay whatever guess wont college useful mine daughter hey realized\n",
      "Topic #1: risk infant baby similar safe western term scientist birth population\n",
      "Topic #2: war lost west united turned history worker respond watch ready\n",
      "Topic #3: twin perhaps walk treatment watching lost disease fear research basis\n",
      "Topic #4: rate showing beyond grow morning stick grade struggling supposed public\n",
      "Topic #5: inside simple piece field move recognize solution video seems choose\n",
      "Topic #6: wouldnt track decision telling voice physical within video familiar advance\n",
      "Topic #7: younger free uncomfortable perfect energy violence sister telling cost write\n",
      "Topic #8: race french nature towards run ok host gone control onto\n",
      "Topic #9: yearold piece ultimately brother hope discovered mountain opportunity electrical everybody\n",
      "Topic #10: quickly taught began met couple stopped immediate death surprised hear\n",
      "Topic #11: father seven lunch wrote john chocolate win honest talked standard\n",
      "Topic #12: culture men tree humanity background seem term neighborhood recently bike\n",
      "Topic #13: walk seeing street website sit hit run continue ground specific\n",
      "Topic #14: institution raising message third hear speak fully sound feeling act\n",
      "\n",
      "Fitting LDA models with tf features, n_samples=2000 and n_features=2000...\n",
      "done in 0.673s.\n",
      "\n",
      "Topics in LDA model:\n",
      "Topic #0: orphanage institution living cambodia support poverty residential volunteer vulnerable generation\n",
      "Topic #1: milk rocket breast father infant baby fly launch flight camera\n",
      "Topic #2: summer college poor public nature empowered class york loss lowincome\n",
      "Topic #3: artist men written film period male voice father menstruation writing\n",
      "Topic #4: dog baby lunch sit lady hypothesis consciousness detector roof training\n",
      "Topic #5: refugee camp leadership living solution war test crisis displaced late\n",
      "Topic #6: comic vision charles medium basic baby page print chair dad\n",
      "Topic #7: college soap business public grade video childhood ride supply model\n",
      "Topic #8: baby fetus language sound statistic period happiness critical pregnant birth\n",
      "Topic #9: men gender feminist culture autism class female baby marriage louis\n",
      "Topic #10: stuff period toy film disney math planet store entrepreneur daughter\n",
      "Topic #11: bee uncertainty film okay paper perception step published puzzle scientist\n",
      "Topic #12: language write writing happy english laptop grade code score teaching\n",
      "Topic #13: nature center build write game class york named classroom relationship\n",
      "Topic #14: game fear sleep health poor playing bank aspiration father baby\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Author: Olivier Grisel <olivier.grisel@ensta.org>\n",
    "#         Lars Buitinck\n",
    "#         Chyi-Kwei Yau <chyikwei.yau@gmail.com>\n",
    "# License: BSD 3 clause\n",
    "\n",
    "from time import time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "n_samples = 2000\n",
    "n_features = 2000\n",
    "n_components = 15\n",
    "n_top_words = 10\n",
    "n_gram = 1\n",
    "alpha = 0.1\n",
    "stop_choice= stop_list\n",
    "\n",
    "max_df = 0.3\n",
    "min_df = 3\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()\n",
    "\n",
    "\n",
    "# Use tf-idf features for NMF.\n",
    "print(\"Extracting tf-idf features for NMF...\")\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=max_df, min_df=min_df,\n",
    "                                   ngram_range=(n_gram,n_gram),\n",
    "                                   max_features=n_features,\n",
    "                                   stop_words=stop_choice)\n",
    "t0 = time()\n",
    "tfidf = tfidf_vectorizer.fit_transform(docs)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "# Use tf (raw term count) features for LDA.\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = CountVectorizer(max_df=max_df, min_df=min_df,\n",
    "                                ngram_range=(n_gram,n_gram),\n",
    "                                max_features=n_features,\n",
    "                                stop_words=stop_choice)\n",
    "t0 = time()\n",
    "tf = tf_vectorizer.fit_transform(docs)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "print()\n",
    "\n",
    "# Fit the NMF model\n",
    "print(\"Fitting the NMF model (Frobenius norm) with tf-idf features, \"\n",
    "      \"n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "t0 = time()\n",
    "nmf = NMF(n_components=n_components, random_state=1,\n",
    "          alpha=alpha, l1_ratio=.5).fit(tfidf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "print(\"\\nTopics in NMF model (Frobenius norm):\")\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words)\n",
    "\n",
    "# Fit the NMF model\n",
    "print(\"Fitting the NMF model (generalized Kullback-Leibler divergence) with \"\n",
    "      \"tf-idf features, n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "t0 = time()\n",
    "nmf = NMF(n_components=n_components, random_state=1,\n",
    "          beta_loss='kullback-leibler', solver='mu', max_iter=1000, alpha=alpha,\n",
    "          l1_ratio=.5).fit(tfidf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "print(\"\\nTopics in NMF model (generalized Kullback-Leibler divergence):\")\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words)\n",
    "\n",
    "print(\"Fitting LDA models with tf features, \"\n",
    "      \"n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "lda = LatentDirichletAllocation(n_components=n_components, max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "t0 = time()\n",
    "lda.fit(tf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "print(\"\\nTopics in LDA model:\")\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic #0: design computer project art machine video object image space book\n",
    "Topic #1: universe planet earth star galaxy mar space light sun solar\n",
    "Topic #2: water energy forest plant oil climate tree carbon fuel river\n",
    "Topic #3: patient cancer disease health doctor drug medical treatment tumor care\n",
    "Topic #4: data information algorithm web phone map computer decision google machine\n",
    "Topic #5: city car street space neighborhood community urban map york architecture\n",
    "Topic #6: brain neuron memory behavior signal activity region pattern consciousness study\n",
    "Topic #7: robot machine leg intelligence animal control task lab computer moving\n",
    "Topic #8: music sound song play hear playing piece listen video played\n",
    "Topic #9: cell dna gene genome virus tissue molecule bacteria disease genetic\n",
    "Topic #10: government company dollar business africa global economy market social economic\n",
    "Topic #11: ocean animal fish sea coral shark specie planet creature ice\n",
    "Topic #12: student teacher education classroom learning class teaching teach math english\n",
    "Topic #13: girl men mother father boy god parent book war shes\n",
    "Topic #14: food plant eat farmer feed animal grow growing healthy waste\n",
    "\n",
    "Topic #0: design computer project machine art video image object space piece\n",
    "Topic #1: universe planet earth star galaxy mar space solar light sun\n",
    "Topic #2: cancer patient disease doctor health drug treatment medical care surgery\n",
    "Topic #3: government company dollar business africa global economy market social economic\n",
    "Topic #4: data information algorithm computer web phone map decision google machine\n",
    "Topic #5: city space street neighborhood community urban design map york architecture\n",
    "Topic #6: robot machine leg intelligence animal control task computer lab moving\n",
    "Topic #7: music sound song play hear playing piece listen video played\n",
    "Topic #8: cell dna gene genome tissue virus molecule disease animal bacteria\n",
    "Topic #9: language english book chinese sound write speak read writing letter\n",
    "Topic #10: water ocean animal sea fish specie forest ice coral earth\n",
    "Topic #11: car vehicle mile road drive oil energy driving ca fuel\n",
    "Topic #12: student teacher education learning class teaching teach math college university\n",
    "Topic #13: girl men mother father boy god parent war shes felt\n",
    "Topic #14: food plant eat farmer feed grow waste growing energy healthy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic #0: men money class film stuff building car fear art american\n",
    "Topic #1: baby statistic ball hypothesis language fetus brain milk evidence birth\n",
    "Topic #2: father dance photograph dad type door watched needed wear authority\n",
    "Topic #3: autism individual gene genetic cause disorder diagnosis condition brain identify\n",
    "Topic #4: grade college successful childhood success score summer wait principle principal\n",
    "Topic #5: refugee camp war parenting coach conflict crisis support grandmother journey\n",
    "Topic #6: game project video code scratch engineering write interactive cancer paper\n",
    "Topic #7: english india computer slum experiment educational google area village internet\n",
    "Topic #8: laptop drop project government rest village teaching television certainly scale\n",
    "Topic #9: brain disorder helmet skull adolescence football injury cortex gray adolescent\n",
    "Topic #10: brother yearold song fell pound excited wed wrote key discovered\n",
    "Topic #11: sex sexual pregnancy pregnant men pleasure partner worth conversation desire\n",
    "Topic #12: lunch food lady breakfast feeding dollar meal chicken billion eat\n",
    "Topic #13: nature forest plant specie tree park wild animal national planet\n",
    "Topic #14: water bottle planet content data method global crisis drink sun\n",
    "\n",
    "Topic #0: class men college grade american stuff art dream film classroom\n",
    "Topic #1: baby statistic ball language evidence fetus milk birth consciousness genetic\n",
    "Topic #2: war arm zone notice dying aid died forced peace democracy\n",
    "Topic #3: autism individual disorder gene diagnosis genetic cause condition treatment expert\n",
    "Topic #4: lunch lady feeding chicken meal billion breakfast feed eat staff\n",
    "Topic #5: refugee camp war coach parenting conflict support crisis grandmother journey\n",
    "Topic #6: game video write cancer engineering interactive classroom code title player\n",
    "Topic #7: laptop government drop rest village teaching television certainly scale ok\n",
    "Topic #8: fire fear energy injury record safety prevent outside began digital\n",
    "Topic #9: india english slum experiment educational google village area internet urban\n",
    "Topic #10: father dance photograph dad door type watched needed wear authority\n",
    "Topic #11: brother yearold song fell pound excited wed key wrote discovered\n",
    "Topic #12: dna monkey gene animal disease stress textbook signal develop poor\n",
    "Topic #13: forest nature plant specie tree park wild animal planet national\n",
    "Topic #14: sex sexual pregnancy pregnant men pleasure partner worth conversation desire\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
